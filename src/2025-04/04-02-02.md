# 使用Bun脚本将静态博客上传至Cloudflare R2

## 项目背景

最近需要将积累的Markdown博客发布为静态网站。经过评估，选择Cloudflare R2作为存储方案，主要优势包括：
- 10GB免费存储空间
- 全球CDN加速
- 兼容S3 API，易于集成

## 技术选型

### 为什么选择Bun
1. 环境因素：当前机器已安装Bun，但未配置Python环境
2. 生态支持：Cloudflare相关库在npm上非常完善
3. 开发效率：Bun内置文件操作和环境变量读取功能

### 核心技术栈
- `@aws-sdk/client-s3`：通过S3兼容API操作Cloudflare R2
- Bun运行时：处理文件操作和环境变量
- 文件哈希比对：避免重复上传相同内容
### 最终结果
- 博客成功发布到Cloudflare R2，并且通过Cloudflare的CDN加速，但是国内速度并不是特别理想，可能后续会迁移到七牛/我的HK云主机套CF。

## 实现方案

### 核心功能
1. **文件遍历**：递归扫描博客目录
2. **哈希比对**：计算文件SHA256哈希，避免重复上传
3. **MIME类型设置**：确保浏览器正确解析文件
4. **断点续传**：通过hash.bin记录已上传文件

### 详细实现

#### 1. S3客户端初始化
``` TypeScript
//配置接口声明：
interface Config {
  endpoint: string;
  credentials: {
    accessKeyId: string;
    secretAccessKey: string;
  };
  region: string;
}
// CF_ACCOUNT_ID, CF_ACCESS_KEY_ID, CF_SECRET_ACCESS_KEY 从.env文件中读取
const config: Config = {
  endpoint: `https://${process.env.CF_ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: process.env.CF_ACCESS_KEY_ID || '',
    secretAccessKey: process.env.CF_SECRET_ACCESS_KEY || '',
  },
  region: "auto",
};
//初始化S3客户端
const client = new S3Client(config);
```

#### 2. MIME类型映射
``` TypeScript
// 根据文件扩展名获取对应的MIME类型
function getContentType(filename: string): string {
  return MIME_TYPES[extname(filename).toLowerCase()] || 'application/octet-stream';
}
const MIME_TYPES: Record<string, string> = {
  '.html': 'text/html',
  '.css': 'text/css',
  '.js': 'application/javascript',
  '.json': 'application/json',
  '.png': 'image/png',
  '.svg': 'image/svg+xml',
  '.jpg': 'image/jpeg',
  '.jpeg': 'image/jpeg',
  '.woff': 'font/woff',
  '.woff2': 'font/woff2',
  '.ttf': 'font/ttf',
  '.eot': 'application/vnd.ms-fontobject',
};
```

#### 3. 文件哈希处理
``` TypeScript
// 这里用一个set存储已经上传的文件hash
let file_hashes: Set<string> = new Set<string>();

async function getFileHash(filePath: string): Promise<string> {
  const content = await Bun.file(filePath).arrayBuffer();
  return createHash('sha256').update(Buffer.from(content)).digest('hex');
}

async function isHashExists(hash: string): Promise<boolean> {
  return file_hashes?.has(hash) || false;
}

async function recordHash(hash: string): Promise<void> {
  file_hashes.add(hash);
}

async function saveHash(): Promise<void> {
  const hash_binFile_path = join(process.cwd(), 'hash.bin');
  await writeFile(hash_binFile_path, [...file_hashes].join('\n'));
}

async function loadHash(): Promise<void> {
  const hash_binFile_path = join(process.cwd(), 'hash.bin');
  const hashes = await readFile(hash_binFile_path, 'utf-8');
  file_hashes = new Set<string>(hashes.split('\n'));
}
```

#### 4. 文件上传核心逻辑
``` TypeScript
async function uploadFile(bucket: string, filePath: string, key: string): Promise<void> {
  const hash = await getFileHash(filePath);

  if (await isHashExists(hash)) {
    console.log(`Skipping ${key} (already uploaded)`);
    return;
  }

  const fileContent = await Bun.file(filePath).arrayBuffer();
  const command = new PutObjectCommand({
    Bucket: bucket,
    Key: key,
    Body: new Uint8Array(fileContent),
    ContentType: getContentType(key)
  });

  await client.send(command);
  await recordHash(hash);
  console.log(`Uploaded ${key} and recorded hash`);
}

// 递归上传整个目录
async function uploadDirectory(bucket: string, dirPath: string): Promise<void> {
  const files = await readdir(dirPath);

  await Promise.all(files.map(async (file) => {
    const fullPath = join(dirPath, file);
    const stats = await stat(fullPath);

    if (stats.isDirectory()) {
      await uploadDirectory(bucket, fullPath);
    } else {
      const relativePath = relative(process.cwd(), fullPath);
      console.log(`Uploading ${relativePath}...`);
      await uploadFile(bucket, fullPath, relativePath);
    }
  }));
}
```

#### 5. 完整脚本
``` TypeScript
#!/usr/bin/env bun
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";
import { readdir, stat, readFile, writeFile } from "fs/promises";
import { join, relative, extname } from "path";
import { createHash } from "crypto";

interface Config {
  endpoint: string;
  credentials: {
    accessKeyId: string;
    secretAccessKey: string;
  };
  region: string;
}
function getContentType(filename: string): string {
  return MIME_TYPES[extname(filename).toLowerCase()] || 'application/octet-stream';
}
const MIME_TYPES: Record<string, string> = {
  '.html': 'text/html',
  '.css': 'text/css',
  '.js': 'application/javascript',
  '.json': 'application/json',
  '.png': 'image/png',
  '.svg': 'image/svg+xml',
  '.jpg': 'image/jpeg',
  '.jpeg': 'image/jpeg',
  '.woff': 'font/woff',
  '.woff2': 'font/woff2',
  '.ttf': 'font/ttf',
  '.eot': 'application/vnd.ms-fontobject',
};

const config: Config = {
  endpoint: `https://${process.env.CF_ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: process.env.CF_ACCESS_KEY_ID || '',
    secretAccessKey: process.env.CF_SECRET_ACCESS_KEY || '',
  },
  region: "auto",
};

const client = new S3Client(config);
const BUCKET_NAME = process.env.CF_BUCKET_NAME || '';

let file_hashes: Set<string> = new Set<string>();

async function getFileHash(filePath: string): Promise<string> {
  const content = await Bun.file(filePath).arrayBuffer();
  return createHash('sha256').update(Buffer.from(content)).digest('hex');
}

async function isHashExists(hash: string): Promise<boolean> {
  return file_hashes?.has(hash) || false;
}

async function recordHash(hash: string): Promise<void> {
  file_hashes.add(hash);
}

async function saveHash(): Promise<void> {
  console.log('Saving hash.bin...');
  const hash_binFile_path = join(process.cwd(), 'hash.bin');
  await writeFile(hash_binFile_path, [...file_hashes].join('\n'));
  console.log('hash.bin saved successfully!');
}

async function loadHash(): Promise<void> {
  console.log('Initializing hash.bin...');
  const hash_binFile_path = join(process.cwd(), 'hash.bin');
  const hashes = await readFile(hash_binFile_path, 'utf-8');
  file_hashes = new Set<string>(hashes.split('\n'));
}

async function uploadFile(bucket: string, filePath: string, key: string): Promise<void> {
  const hash = await getFileHash(filePath);

  if (await isHashExists(hash)) {
    console.log(`Skipping ${key} (already uploaded)`);
    return;
  }

  const fileContent = await Bun.file(filePath).arrayBuffer();
  const command = new PutObjectCommand({
    Bucket: bucket,
    Key: key,
    Body: new Uint8Array(fileContent),
    ContentType: getContentType(key)
  });

  await client.send(command);
  await recordHash(hash);
  console.log(`Uploaded ${key} and recorded hash`);
}

async function uploadDirectory(bucket: string, dirPath: string): Promise<void> {
  console.log(`Uploading ${dirPath}...`);
  const files = await readdir(dirPath);

  await Promise.all(files.map(async (file) => {
    const fullPath = join(dirPath, file);
    const stats = await stat(fullPath);

    if (stats.isDirectory()) {
      await uploadDirectory(bucket, fullPath);
    } else {
      const relativePath = relative(process.cwd(), fullPath);
      console.log(`Uploading ${relativePath}...`);
      await uploadFile(bucket, fullPath, relativePath);
    }
  }));
}

async function main(): Promise<void> {
  if (!BUCKET_NAME) {
    console.error('Please set CF_BUCKET_NAME environment variable');
    process.exit(1);
  }
  try {
    await loadHash();
  } catch (error) {
    console.error('Failed to read hash.bin:', error);
    process.exit(1);
  }

  try {
    await uploadDirectory(BUCKET_NAME, './book');
    console.log('Upload completed successfully!');
    await saveHash();
  } catch (error) {
    console.error('Upload failed:', error);
    process.exit(1);
  }
}

main();
```

## 使用指南

### 准备工作
1. 在Cloudflare R2中创建Bucket
2. 生成Access Key ID和Secret Access Key
3. 将凭证写入`.env`文件

### 执行上传
```bash
bun i  # 安装依赖
bun run upload_blog.ts  # 执行上传
```

### 后续配置
1. **公开访问**：在R2控制台设置Bucket为公开访问
2. **自定义域名**：可绑定到自己的子域名
3. **缓存设置**：在Cloudflare缓存规则中添加路径模式（如`https://blog.xlfish233.win/*`），初始可设置为1天缓存
